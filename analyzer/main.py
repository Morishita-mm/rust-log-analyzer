import redis
import json
import time
import polars as pl

# Redis conf
REDIS_HOST = 'redis'
REDIS_PORT = 6379
LOGS_CHANNEL = 'logs.ingest'
STATS_CHANNEL = 'stats.update'
BUFFER_DURATION_SEC = 1.0   # é›†è¨ˆã®é–“éš”


def process_logs(logs_buffer):
    """
    ãƒãƒƒãƒ•ã‚¡ã«æºœã¾ã£ãŸãƒ­ã‚°ã‚’Polarsã§é›†è¨ˆã—ã€çµæžœã‚’Redisã«é€ä¿¡ã™ã‚‹
    """
    if not logs_buffer:
        return

    try:
        # 1. è¾žæ›¸ã®ãƒªã‚¹ãƒˆã‹ã‚‰Polars DataFrameã‚’ä½œæˆ
        df = pl.DataFrame(logs_buffer)
        df = df.with_columns(pl.col("timestamp").str.to_datetime(time_zone="UTC"))
        
        # 3. é›†è¨ˆå‡¦ç†ï¼ˆgroup_by_dynamicã§æ™‚é–“ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†è¨ˆï¼‰
        aggregated_df = df.group_by_dynamic("timestamp", every="1s").agg([pl.len().alias("total_count").cast(pl.Int64), # æœŸé–“å†…ã®ç·ãƒ­ã‚°æ•°
                                                                          (pl.col("level") == "ERROR").sum().alias(
                                                                              "error_count").cast(pl.Int64),  # æœŸé–“å†…ã®ã‚¨ãƒ©ãƒ¼å›žæ•°
                                                                          pl.col("service").mode().first().alias(
                                                                              "top_service")     # æœ€ã‚‚ãƒ­ã‚°ã®å‡ºåŠ›ãŒå¤šã‹ã£ãŸã‚µãƒ¼ãƒ“ã‚¹
                                                                          ])
        aggregated_df = aggregated_df.with_columns([
            pl.col("timestamp").alias("window_start"),
            (pl.col("timestamp") + pl.duration(seconds=1)).alias("window_end")
        ])

        # ä¸è¦ã«ãªã£ãŸå…ƒã® 'timestamp' åˆ—ã‚’é™¤å¤–ã—ã€åˆ—ã®é †åºã‚’æ•´ç†ã—ã¾ã™ï¼ˆRustå´ã®æ§‹é€ ä½“ã«åˆã‚ã›ã‚‹ãŸã‚ï¼‰
        aggregated_df = aggregated_df.select([
            "window_start",
            "window_end",
            "total_count",
            "error_count",
            "top_service"
        ])

        # 4. é›†è¨ˆçµæžœã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
        stats_json = aggregated_df.write_json()

        # 5. Redisã«é€ä¿¡ï¼ˆRustãŒè³¼èª­ã—ã¦ã„ã‚‹ãƒãƒ£ãƒ³ãƒãƒ«ã¸ï¼‰
        # TODO: Redisã¨ã®æŽ¥ç¶šã¯å¼•æ•°ã§æ¸¡ã™ã€ã‚‚ã—ãã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§å®šç¾©ã™ã‚‹ã‚ˆã†ã«å¤‰æ›´ã™ã‚‹
        client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
        client.publish(STATS_CHANNEL, stats_json)
        print(f"ðŸ“Š Sent stats update: {stats_json}")
    except Exception as e:
        print(f"âŒ Error processing logs: {e}")


def main():
    try:
        client = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
        pubsub = client.pubsub()
        pubsub.subscribe(LOGS_CHANNEL)
        client.ping()
        print(f"âœ… Connected to Redis. Subscribed to {LOGS_CHANNEL}")
    except redis.ConnectionError as e:
        print(f"âŒ Could not connect to Redis: {e}")
        return

    print(f"ðŸš€ Starting dummy log publisher to channel '{LOGS_CHANNEL}'...")

    logs_buffer = []
    last_process_time = time.time()

    try:
        while True:
            message = pubsub.get_message(timeout=0.1)

            if message and message['type'] == 'message':
                try:
                    log_data = json.loads(message['data'])
                    logs_buffer.append(log_data)
                except json.JSONDecodeError:
                    print(f"âŒ Received invalid JSON on {LOGS_CHANNEL}")

            current_time = time.time()
            if current_time - last_process_time >= BUFFER_DURATION_SEC:
                if logs_buffer:
                    if logs_buffer:
                        process_logs(logs_buffer)
                        logs_buffer = []
                    last_process_time = current_time

    except KeyboardInterrupt:
        print("\nðŸ”´ Log analyzer stopped.")
        pubsub.close()


if __name__ == "__main__":
    main()
